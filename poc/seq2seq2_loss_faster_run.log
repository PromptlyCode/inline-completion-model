(base) ➜  poc git:(main) ✗ du -sh best_translator.pth 
386M	best_translator.pth
(base) ➜  poc git:(main) ✗

(inline-completion-model) ➜  poc git:(main) ✗ prunp seq2seq2_loss_faster.py
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.807 seconds.
Prefix dict has been built successfully.
/home/xlisp/anaconda3/envs/inline-completion-model/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Epoch [1/20], Batch [0/1562], Loss: 10.4329, Avg Loss: 10.4329
Epoch [1/20], Batch [50/1562], Loss: 7.7123, Avg Loss: 8.1734
Epoch [1/20], Batch [100/1562], Loss: 7.5982, Avg Loss: 7.6324
Epoch [1/20], Batch [150/1562], Loss: 7.5850, Avg Loss: 7.5793
Epoch [1/20], Batch [200/1562], Loss: 7.4222, Avg Loss: 7.5482
Epoch [1/20], Batch [250/1562], Loss: 7.4220, Avg Loss: 7.5166
Epoch [1/20], Batch [300/1562], Loss: 7.5148, Avg Loss: 7.4794
Epoch [1/20], Batch [350/1562], Loss: 7.4495, Avg Loss: 7.4678
Epoch [1/20], Batch [400/1562], Loss: 7.5150, Avg Loss: 7.4347
Epoch [1/20], Batch [450/1562], Loss: 7.4333, Avg Loss: 7.4238
Epoch [1/20], Batch [500/1562], Loss: 7.4033, Avg Loss: 7.4233
Epoch [1/20], Batch [550/1562], Loss: 7.3593, Avg Loss: 7.3910
Epoch [1/20], Batch [600/1562], Loss: 7.4481, Avg Loss: 7.3767
Epoch [1/20], Batch [650/1562], Loss: 7.3853, Avg Loss: 7.3792
Epoch [1/20], Batch [700/1562], Loss: 7.3144, Avg Loss: 7.3813
Epoch [1/20], Batch [750/1562], Loss: 7.3315, Avg Loss: 7.3444
Epoch [1/20], Batch [800/1562], Loss: 7.2417, Avg Loss: 7.3364
Epoch [1/20], Batch [850/1562], Loss: 7.3738, Avg Loss: 7.3411
Epoch [1/20], Batch [900/1562], Loss: 7.2536, Avg Loss: 7.3235
Epoch [1/20], Batch [950/1562], Loss: 7.3010, Avg Loss: 7.3067
Epoch [1/20], Batch [1000/1562], Loss: 7.3853, Avg Loss: 7.2944
Epoch [1/20], Batch [1050/1562], Loss: 7.0715, Avg Loss: 7.2843
Epoch [1/20], Batch [1100/1562], Loss: 7.2385, Avg Loss: 7.2679
Epoch [1/20], Batch [1150/1562], Loss: 7.3661, Avg Loss: 7.2856
Epoch [1/20], Batch [1200/1562], Loss: 7.2997, Avg Loss: 7.2573
Epoch [1/20], Batch [1250/1562], Loss: 7.3177, Avg Loss: 7.2463
Epoch [1/20], Batch [1300/1562], Loss: 7.1893, Avg Loss: 7.2334
Epoch [1/20], Batch [1350/1562], Loss: 7.1841, Avg Loss: 7.2535
Epoch [1/20], Batch [1400/1562], Loss: 7.2610, Avg Loss: 7.2279
Epoch [1/20], Batch [1450/1562], Loss: 7.2292, Avg Loss: 7.2158
Epoch [1/20], Batch [1500/1562], Loss: 7.2343, Avg Loss: 7.2256
Epoch [1/20], Batch [1550/1562], Loss: 7.2202, Avg Loss: 7.1930
Epoch: 1, Average Loss: 7.3831
Model saved to best_translator.pth
New best model saved with loss: 7.3831
Epoch [2/20], Batch [0/1562], Loss: 7.1694, Avg Loss: 7.1694
Epoch [2/20], Batch [50/1562], Loss: 7.2463, Avg Loss: 7.1641
Epoch [2/20], Batch [100/1562], Loss: 7.1279, Avg Loss: 7.1828
Epoch [2/20], Batch [150/1562], Loss: 7.3905, Avg Loss: 7.1749
Epoch [2/20], Batch [200/1562], Loss: 7.2100, Avg Loss: 7.1723
Epoch [2/20], Batch [250/1562], Loss: 7.2544, Avg Loss: 7.1582
Epoch [2/20], Batch [300/1562], Loss: 7.0413, Avg Loss: 7.1573
Epoch [2/20], Batch [350/1562], Loss: 7.1976, Avg Loss: 7.1361
Epoch [2/20], Batch [400/1562], Loss: 7.1731, Avg Loss: 7.1504
Epoch [2/20], Batch [450/1562], Loss: 7.2324, Avg Loss: 7.1365
Epoch [2/20], Batch [500/1562], Loss: 7.1592, Avg Loss: 7.1410
Epoch [2/20], Batch [550/1562], Loss: 7.2080, Avg Loss: 7.1528
Epoch [2/20], Batch [600/1562], Loss: 7.0708, Avg Loss: 7.1411
Epoch [2/20], Batch [650/1562], Loss: 7.0365, Avg Loss: 7.1274
Epoch [2/20], Batch [700/1562], Loss: 7.0481, Avg Loss: 7.1069
Epoch [2/20], Batch [750/1562], Loss: 7.0272, Avg Loss: 7.1149
Epoch [2/20], Batch [800/1562], Loss: 7.1781, Avg Loss: 7.1026
Epoch [2/20], Batch [850/1562], Loss: 7.1809, Avg Loss: 7.1242
Epoch [2/20], Batch [900/1562], Loss: 6.9973, Avg Loss: 7.1086
Epoch [2/20], Batch [950/1562], Loss: 7.0742, Avg Loss: 7.1275
Epoch [2/20], Batch [1000/1562], Loss: 7.0563, Avg Loss: 7.1322
Epoch [2/20], Batch [1050/1562], Loss: 7.0766, Avg Loss: 7.0863
Epoch [2/20], Batch [1100/1562], Loss: 7.2784, Avg Loss: 7.0715
Epoch [2/20], Batch [1150/1562], Loss: 7.1199, Avg Loss: 7.0947
Epoch [2/20], Batch [1200/1562], Loss: 7.1529, Avg Loss: 7.1247
Epoch [2/20], Batch [1250/1562], Loss: 7.0954, Avg Loss: 7.0741
Epoch [2/20], Batch [1300/1562], Loss: 7.0230, Avg Loss: 7.0822
Epoch [2/20], Batch [1350/1562], Loss: 6.9680, Avg Loss: 7.0534
Epoch [2/20], Batch [1400/1562], Loss: 7.0573, Avg Loss: 7.0729
Epoch [2/20], Batch [1450/1562], Loss: 7.0592, Avg Loss: 7.0721
Epoch [2/20], Batch [1500/1562], Loss: 7.0226, Avg Loss: 7.0524
Epoch [2/20], Batch [1550/1562], Loss: 6.9905, Avg Loss: 7.0776
Epoch: 2, Average Loss: 7.1184
Model saved to best_translator.pth
New best model saved with loss: 7.1184
Epoch [3/20], Batch [0/1562], Loss: 6.9227, Avg Loss: 6.9227
Epoch [3/20], Batch [50/1562], Loss: 7.0949, Avg Loss: 7.0242
Epoch [3/20], Batch [100/1562], Loss: 7.0032, Avg Loss: 7.0364
Epoch [3/20], Batch [150/1562], Loss: 6.9491, Avg Loss: 7.0231
Epoch [3/20], Batch [200/1562], Loss: 6.9764, Avg Loss: 7.0312
Epoch [3/20], Batch [250/1562], Loss: 6.8470, Avg Loss: 7.0392
Epoch [3/20], Batch [300/1562], Loss: 6.8656, Avg Loss: 7.0155
Epoch [3/20], Batch [350/1562], Loss: 7.0722, Avg Loss: 7.0095
Epoch [3/20], Batch [400/1562], Loss: 7.2003, Avg Loss: 7.0113
Epoch [3/20], Batch [450/1562], Loss: 6.9705, Avg Loss: 7.0094
Epoch [3/20], Batch [500/1562], Loss: 6.8672, Avg Loss: 7.0195
Epoch [3/20], Batch [550/1562], Loss: 6.7716, Avg Loss: 6.9956
Epoch [3/20], Batch [600/1562], Loss: 7.0181, Avg Loss: 7.0214
... ...
Epoch [10/20], Batch [1550/1562], Loss: 6.6842, Avg Loss: 6.7122
Epoch: 10, Average Loss: 6.6804
Model saved to best_translator.pth
New best model saved with loss: 6.6804
Model saved to translator_checkpoint_epoch_10.pth
Epoch [11/20], Batch [0/1562], Loss: 6.7575, Avg Loss: 6.7575
Epoch [11/20], Batch [50/1562], Loss: 6.5071, Avg Loss: 6.6137
Epoch [11/20], Batch [100/1562], Loss: 6.2012, Avg Loss: 6.6347
Epoch [11/20], Batch [150/1562], Loss: 6.6668, Avg Loss: 6.6310
Epoch [11/20], Batch [200/1562], Loss: 6.4340, Avg Loss: 6.6270
Epoch [11/20], Batch [250/1562], Loss: 6.5785, Avg Loss: 6.6289
Epoch [11/20], Batch [300/1562], Loss: 6.9681, Avg Loss: 6.6778
Epoch [11/20], Batch [350/1562], Loss: 6.6842, Avg Loss: 6.6661
Epoch [11/20], Batch [400/1562], Loss: 6.6927, Avg Loss: 6.6633
Epoch [11/20], Batch [450/1562], Loss: 6.5682, Avg Loss: 6.6335
Epoch [11/20], Batch [500/1562], Loss: 6.6451, Avg Loss: 6.6207
Epoch [11/20], Batch [550/1562], Loss: 6.6597, Avg Loss: 6.6206
Epoch [11/20], Batch [600/1562], Loss: 6.5039, Avg Loss: 6.6295
Epoch [11/20], Batch [650/1562], Loss: 6.6399, Avg Loss: 6.6393
Epoch [11/20], Batch [700/1562], Loss: 6.6697, Avg Loss: 6.6589
Epoch [11/20], Batch [750/1562], Loss: 6.5500, Avg Loss: 6.6583
Epoch [11/20], Batch [800/1562], Loss: 6.8268, Avg Loss: 6.6444
Epoch [11/20], Batch [850/1562], Loss: 6.7426, Avg Loss: 6.6311
Epoch [11/20], Batch [900/1562], Loss: 6.4077, Avg Loss: 6.6289
Epoch [11/20], Batch [950/1562], Loss: 6.7412, Avg Loss: 6.6935
Epoch [11/20], Batch [1000/1562], Loss: 6.7015, Avg Loss: 6.6482
Epoch [11/20], Batch [1050/1562], Loss: 6.5469, Avg Loss: 6.6639
Epoch [11/20], Batch [1100/1562], Loss: 6.5700, Avg Loss: 6.6862
Epoch [11/20], Batch [1150/1562], Loss: 6.7651, Avg Loss: 6.6514
Epoch [11/20], Batch [1200/1562], Loss: 6.8476, Avg Loss: 6.6475
Epoch [11/20], Batch [1250/1562], Loss: 6.2904, Avg Loss: 6.6350
Epoch [11/20], Batch [1300/1562], Loss: 6.6531, Avg Loss: 6.6425
Epoch [11/20], Batch [1350/1562], Loss: 6.6183, Avg Loss: 6.6601
Epoch [11/20], Batch [1400/1562], Loss: 6.7930, Avg Loss: 6.6592
Epoch [11/20], Batch [1450/1562], Loss: 6.5637, Avg Loss: 6.6171
Epoch [11/20], Batch [1500/1562], Loss: 6.3216, Avg Loss: 6.6685
Epoch [11/20], Batch [1550/1562], Loss: 6.8497, Avg Loss: 6.6691
Epoch: 11, Average Loss: 6.6468
Model saved to best_translator.pth
New best model saved with loss: 6.6468
Epoch [12/20], Batch [0/1562], Loss: 6.6654, Avg Loss: 6.6654
Epoch [12/20], Batch [50/1562], Loss: 6.6277, Avg Loss: 6.6155
Epoch [12/20], Batch [100/1562], Loss: 6.6326, Avg Loss: 6.5877
Epoch [12/20], Batch [150/1562], Loss: 6.7460, Avg Loss: 6.6092
Epoch [12/20], Batch [200/1562], Loss: 6.9692, Avg Loss: 6.6127
Epoch [12/20], Batch [250/1562], Loss: 6.6560, Avg Loss: 6.6406
Epoch [12/20], Batch [300/1562], Loss: 6.8133, Avg Loss: 6.6327
Epoch [12/20], Batch [350/1562], Loss: 6.6403, Avg Loss: 6.5693
Epoch [12/20], Batch [400/1562], Loss: 6.9665, Avg Loss: 6.6126
Epoch [12/20], Batch [450/1562], Loss: 6.3245, Avg Loss: 6.6062
Epoch [12/20], Batch [500/1562], Loss: 6.4632, Avg Loss: 6.5769
Epoch [12/20], Batch [550/1562], Loss: 6.6177, Avg Loss: 6.6136
Epoch [12/20], Batch [600/1562], Loss: 6.5429, Avg Loss: 6.6420
Epoch [12/20], Batch [650/1562], Loss: 6.8122, Avg Loss: 6.6348
Epoch [12/20], Batch [700/1562], Loss: 6.5358, Avg Loss: 6.6554
Epoch [12/20], Batch [750/1562], Loss: 6.9487, Avg Loss: 6.6327
Epoch [12/20], Batch [800/1562], Loss: 6.5969, Avg Loss: 6.6245
Epoch [12/20], Batch [850/1562], Loss: 6.7673, Avg Loss: 6.6242
Epoch [12/20], Batch [900/1562], Loss: 6.8588, Avg Loss: 6.6397
Epoch [12/20], Batch [950/1562], Loss: 6.6897, Avg Loss: 6.6040
Epoch [12/20], Batch [1000/1562], Loss: 6.5015, Avg Loss: 6.6591
Epoch [12/20], Batch [1350/1562], Loss: 6.5474, Avg Loss: 6.6357
Epoch [12/20], Batch [1400/1562], Loss: 6.6209, Avg Loss: 6.5734
Epoch [12/20], Batch [1450/1562], Loss: 6.3593, Avg Loss: 6.6508
Epoch [12/20], Batch [1500/1562], Loss: 6.7067, Avg Loss: 6.6658
Epoch [12/20], Batch [1550/1562], Loss: 6.6676, Avg Loss: 6.6431
Epoch: 12, Average Loss: 6.6244
Model saved to best_translator.pth
New best model saved with loss: 6.6244
Epoch [13/20], Batch [0/1562], Loss: 6.6578, Avg Loss: 6.6578
Epoch [13/20], Batch [50/1562], Loss: 6.4899, Avg Loss: 6.5985
Epoch [13/20], Batch [100/1562], Loss: 6.5922, Avg Loss: 6.5604
Epoch [13/20], Batch [150/1562], Loss: 6.2504, Avg Loss: 6.5709
Epoch [13/20], Batch [200/1562], Loss: 6.5264, Avg Loss: 6.5986

### ---- test is very bad ... 
In [3]: device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

In [4]: model, optimizer, eng_vocab, chi_vocab, epoch, loss = load_model('best_translator.pth', device)
<ipython-input-2-c56496c046f5>:160: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(filename, map_location=device)

In [5]: translate_sentence(model, "hi", eng_vocab, chi_vocab, device)
Out[5]: '<unk>的<unk>。'

In [6]: translate_sentence(model, "Who are you", eng_vocab, chi_vocab, device)
Out[6]: '在，的的的。'

In [7]: translate_sentence(model, "ok", eng_vocab, chi_vocab, device)
Out[7]: '<unk>的<unk>。'

In [8]: translate_sentence(model, "Of course, the fall of the house of Lehman", eng_vocab, chi_vocab, device)
Out[8]: '在，，，的的的的的。'

In [9]: translate_sentence(model, "You actually have to implement the solution", eng_vocab, chi_vocab, device)
Out[9]: '因此，在的的的的。'

In [10]:

