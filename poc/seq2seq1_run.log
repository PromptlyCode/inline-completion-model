(inline-completion-model) ➜  poc git:(main) ✗ prunp seq2seq1.py
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.789 seconds.
Prefix dict has been built successfully.
Epoch [1/10], Batch [0/7111], Loss: 10.8502
Epoch [1/10], Batch [100/7111], Loss: 7.4877
Epoch [1/10], Batch [200/7111], Loss: 7.2134
Epoch [1/10], Batch [300/7111], Loss: 7.0276
Epoch [1/10], Batch [400/7111], Loss: 7.2494
Epoch [1/10], Batch [500/7111], Loss: 7.0450
Epoch [1/10], Batch [600/7111], Loss: 6.8434
Epoch [1/10], Batch [700/7111], Loss: 6.8179
Epoch [1/10], Batch [800/7111], Loss: 6.7652
Epoch [1/10], Batch [900/7111], Loss: 7.0036
Epoch [1/10], Batch [1000/7111], Loss: 6.8505
Epoch [1/10], Batch [1100/7111], Loss: 6.8977
Epoch [1/10], Batch [1200/7111], Loss: 6.9355
Epoch [1/10], Batch [1300/7111], Loss: 6.8444
Epoch [1/10], Batch [1400/7111], Loss: 6.9839
Epoch [1/10], Batch [1500/7111], Loss: 7.0312
Epoch [1/10], Batch [1600/7111], Loss: 6.8220
Epoch [1/10], Batch [1700/7111], Loss: 6.8934
Epoch [1/10], Batch [1800/7111], Loss: 6.8535
Epoch [1/10], Batch [1900/7111], Loss: 7.0794
Epoch [1/10], Batch [2000/7111], Loss: 6.6744
Epoch [1/10], Batch [2100/7111], Loss: 6.7432
Epoch [1/10], Batch [2200/7111], Loss: 7.1960
Epoch [1/10], Batch [2300/7111], Loss: 6.9062
Epoch [1/10], Batch [2400/7111], Loss: 7.0247
Epoch [1/10], Batch [2500/7111], Loss: 6.6774
Epoch [1/10], Batch [2600/7111], Loss: 6.7646
Epoch [1/10], Batch [2700/7111], Loss: 6.5998
Epoch [1/10], Batch [2800/7111], Loss: 6.8858
Epoch [1/10], Batch [2900/7111], Loss: 6.9287
Epoch [1/10], Batch [3000/7111], Loss: 6.6883
Epoch [1/10], Batch [3100/7111], Loss: 6.6929
Epoch [1/10], Batch [3200/7111], Loss: 6.2865
Epoch [1/10], Batch [3300/7111], Loss: 6.6183
Epoch [1/10], Batch [3400/7111], Loss: 6.7202
Epoch [1/10], Batch [3500/7111], Loss: 6.3319
Epoch [1/10], Batch [3600/7111], Loss: 6.4363
Epoch [1/10], Batch [3700/7111], Loss: 6.5134
Epoch [1/10], Batch [3800/7111], Loss: 6.7194
Epoch [1/10], Batch [3900/7111], Loss: 6.6766
Epoch [1/10], Batch [4000/7111], Loss: 6.7379
Epoch [1/10], Batch [4100/7111], Loss: 6.4973
Traceback (most recent call last):
  File "/home/xlisp/Desktop/inline-completion-model/poc/seq2seq1.py", line 314, in <module>
    main()
  File "/home/xlisp/Desktop/inline-completion-model/poc/seq2seq1.py", line 301, in main
    train_model()
  File "/home/xlisp/Desktop/inline-completion-model/poc/seq2seq1.py", line 276, in train_model
    loss.backward()
  File "/home/xlisp/anaconda3/envs/inline-completion-model/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/xlisp/anaconda3/envs/inline-completion-model/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/xlisp/anaconda3/envs/inline-completion-model/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.78 GiB. GPU 0 has a total capacity of 7.92 GiB of which 919.00 MiB is free. Including non-PyTorch memory, this process has 6.84 GiB memory in use. Of the allocated memory 6.41 GiB is allocated by PyTorch, and 304.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(inline-completion-model) ➜  poc git:(main) ✗

# -------------- limit 3000 --------------------
(inline-completion-model) ➜  poc git:(main) ✗ prunp seq2seq1.py 
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.802 seconds.
Prefix dict has been built successfully.
Epoch [1/10], Batch [0/93], Loss: 8.4353
Epoch: 1, Average Loss: 6.6387
Model saved to best_translator.pth
Epoch [2/10], Batch [0/93], Loss: 6.3334
Epoch: 2, Average Loss: 6.2941
Model saved to best_translator.pth
Epoch [3/10], Batch [0/93], Loss: 6.2760
Epoch: 3, Average Loss: 6.2026
Model saved to best_translator.pth
Epoch [4/10], Batch [0/93], Loss: 6.1479
Epoch: 4, Average Loss: 6.1310
Model saved to best_translator.pth
Epoch [5/10], Batch [0/93], Loss: 5.8996
Epoch: 5, Average Loss: 6.0701
Model saved to best_translator.pth
Model saved to translator_checkpoint_epoch_5.pth
Epoch [6/10], Batch [0/93], Loss: 5.9100
Epoch: 6, Average Loss: 6.0217
Model saved to best_translator.pth
Epoch [7/10], Batch [0/93], Loss: 5.9804
Epoch: 7, Average Loss: 5.9731
Model saved to best_translator.pth
Epoch [8/10], Batch [0/93], Loss: 5.9093
Epoch: 8, Average Loss: 5.9240
Model saved to best_translator.pth
Epoch [9/10], Batch [0/93], Loss: 5.8466
Epoch: 9, Average Loss: 5.8861
Model saved to best_translator.pth
Epoch [10/10], Batch [0/93], Loss: 5.7643
Epoch: 10, Average Loss: 5.8513
Model saved to best_translator.pth
Model saved to translator_checkpoint_epoch_10.pth
/home/xlisp/Desktop/inline-completion-model/poc/seq2seq1.py:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(filename, map_location=device)
English: The weather is very nice today.
Chinese: <unk>的<unk>。
(inline-completion-model) ➜  poc git:(main) ✗ 

## --------------- limit 30000 ----------------
(inline-completion-model) ➜  poc git:(main) ✗ prunp seq2seq1.py 
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.812 seconds.
Prefix dict has been built successfully.
Epoch [1/10], Batch [0/937], Loss: 9.8325
Epoch [1/10], Batch [100/937], Loss: 7.0746
Epoch [1/10], Batch [200/937], Loss: 7.0262
Epoch [1/10], Batch [300/937], Loss: 6.7384
Epoch [1/10], Batch [400/937], Loss: 6.8841
Epoch [1/10], Batch [500/937], Loss: 6.8259
Epoch [1/10], Batch [600/937], Loss: 6.7976
Epoch [1/10], Batch [700/937], Loss: 6.5541
Epoch [1/10], Batch [800/937], Loss: 6.8184
Epoch [1/10], Batch [900/937], Loss: 6.7321
Epoch: 1, Average Loss: 6.8586
Model saved to best_translator.pth
Epoch [2/10], Batch [0/937], Loss: 6.6374
Epoch [2/10], Batch [100/937], Loss: 6.4108
Epoch [2/10], Batch [200/937], Loss: 6.5541
Epoch [2/10], Batch [300/937], Loss: 6.4729
Epoch [2/10], Batch [400/937], Loss: 6.6500
Epoch [2/10], Batch [500/937], Loss: 6.6287
Epoch [2/10], Batch [600/937], Loss: 6.7331
Epoch [2/10], Batch [700/937], Loss: 6.6707
Epoch [2/10], Batch [800/937], Loss: 6.6965
Epoch [2/10], Batch [900/937], Loss: 6.3086
Epoch: 2, Average Loss: 6.5783
Model saved to best_translator.pth
Epoch [3/10], Batch [0/937], Loss: 6.4978
Epoch [3/10], Batch [100/937], Loss: 6.4654
Epoch [3/10], Batch [200/937], Loss: 6.3915
Epoch [3/10], Batch [300/937], Loss: 6.4935
Epoch [3/10], Batch [400/937], Loss: 6.5444
Epoch [3/10], Batch [500/937], Loss: 6.4667
Epoch [3/10], Batch [600/937], Loss: 6.5002
Epoch [3/10], Batch [700/937], Loss: 6.4216
Epoch [3/10], Batch [800/937], Loss: 6.6347
Epoch [3/10], Batch [900/937], Loss: 6.6197
Epoch: 3, Average Loss: 6.4464
Model saved to best_translator.pth
Epoch [4/10], Batch [0/937], Loss: 6.1031
Epoch [4/10], Batch [100/937], Loss: 6.2837
Epoch [4/10], Batch [200/937], Loss: 6.2713
Epoch [4/10], Batch [300/937], Loss: 6.4337
Epoch [4/10], Batch [400/937], Loss: 6.3171
Epoch [4/10], Batch [500/937], Loss: 6.3637
Epoch [4/10], Batch [600/937], Loss: 6.4535
Epoch [4/10], Batch [700/937], Loss: 6.2667
Epoch [4/10], Batch [800/937], Loss: 6.3794
Epoch [4/10], Batch [900/937], Loss: 6.4462
Epoch: 4, Average Loss: 6.3342
Model saved to best_translator.pth
Epoch [5/10], Batch [0/937], Loss: 6.2940
Epoch [5/10], Batch [100/937], Loss: 6.4471
Epoch [5/10], Batch [200/937], Loss: 6.2905
Epoch [5/10], Batch [300/937], Loss: 6.2058
Epoch [5/10], Batch [400/937], Loss: 6.1677
Epoch [5/10], Batch [500/937], Loss: 6.3148
Epoch [5/10], Batch [600/937], Loss: 6.2171
Epoch [5/10], Batch [700/937], Loss: 6.3209
Epoch [5/10], Batch [800/937], Loss: 6.1877
Epoch [5/10], Batch [900/937], Loss: 6.2006
Epoch: 5, Average Loss: 6.2399
Model saved to best_translator.pth
Model saved to translator_checkpoint_epoch_5.pth
Epoch [6/10], Batch [0/937], Loss: 6.3406
Epoch [6/10], Batch [100/937], Loss: 6.2132
Epoch [6/10], Batch [200/937], Loss: 5.9255
Epoch [6/10], Batch [300/937], Loss: 6.1918
Epoch [6/10], Batch [400/937], Loss: 6.1753
Epoch [6/10], Batch [500/937], Loss: 6.1283
Epoch [6/10], Batch [600/937], Loss: 6.1420
Epoch [6/10], Batch [700/937], Loss: 6.1438
Epoch [6/10], Batch [800/937], Loss: 6.2386
Epoch [6/10], Batch [900/937], Loss: 6.2031
Epoch: 6, Average Loss: 6.1524
Model saved to best_translator.pth
Epoch [7/10], Batch [0/937], Loss: 5.8963
Epoch [7/10], Batch [100/937], Loss: 6.0804
Epoch [7/10], Batch [200/937], Loss: 6.0323
Epoch [7/10], Batch [300/937], Loss: 6.0772
Epoch [7/10], Batch [400/937], Loss: 5.8859
Epoch [7/10], Batch [500/937], Loss: 5.7728
Epoch [7/10], Batch [600/937], Loss: 6.1868
Epoch [7/10], Batch [700/937], Loss: 6.0698
Epoch [7/10], Batch [800/937], Loss: 6.1111
Epoch [7/10], Batch [900/937], Loss: 5.9156
Epoch: 7, Average Loss: 6.0715
Model saved to best_translator.pth
Epoch [8/10], Batch [0/937], Loss: 6.0518
Epoch [8/10], Batch [100/937], Loss: 5.8611
Epoch [8/10], Batch [200/937], Loss: 6.0243
Epoch [8/10], Batch [300/937], Loss: 6.0813
Epoch [8/10], Batch [400/937], Loss: 5.7745
Epoch [8/10], Batch [500/937], Loss: 6.0679
Epoch [8/10], Batch [600/937], Loss: 5.9970
Epoch [8/10], Batch [700/937], Loss: 6.0133
Epoch [8/10], Batch [800/937], Loss: 5.7635
Epoch [8/10], Batch [900/937], Loss: 5.8396
Epoch: 8, Average Loss: 5.9975
Model saved to best_translator.pth
Epoch [9/10], Batch [0/937], Loss: 5.9193
Epoch [9/10], Batch [100/937], Loss: 5.7352
Epoch [9/10], Batch [200/937], Loss: 6.0901
Epoch [9/10], Batch [300/937], Loss: 5.9588
Epoch [9/10], Batch [400/937], Loss: 6.0584
Epoch [9/10], Batch [500/937], Loss: 5.9428
Epoch [9/10], Batch [600/937], Loss: 5.9311
Epoch [9/10], Batch [700/937], Loss: 6.0372
Epoch [9/10], Batch [800/937], Loss: 6.2165
Epoch [9/10], Batch [900/937], Loss: 5.8180
Epoch: 9, Average Loss: 5.9344
Model saved to best_translator.pth
Epoch [10/10], Batch [0/937], Loss: 5.9228
Epoch [10/10], Batch [100/937], Loss: 5.8266
Epoch [10/10], Batch [200/937], Loss: 5.6713
Epoch [10/10], Batch [300/937], Loss: 5.7082
Epoch [10/10], Batch [400/937], Loss: 5.9953
Epoch [10/10], Batch [500/937], Loss: 5.9540
Epoch [10/10], Batch [600/937], Loss: 5.5805
Epoch [10/10], Batch [700/937], Loss: 5.9599
Epoch [10/10], Batch [800/937], Loss: 5.9662
Epoch [10/10], Batch [900/937], Loss: 5.7702
Epoch: 10, Average Loss: 5.8692
Model saved to best_translator.pth
Model saved to translator_checkpoint_epoch_10.pth
/home/xlisp/Desktop/inline-completion-model/poc/seq2seq1.py:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(filename, map_location=device)
English: The weather is very nice today.
Chinese: 这的。
(inline-completion-model) ➜  poc git:(main) ✗ 

## ---- test ---- 

In [1]: # %load seq2seq1.py

In [2]: device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

In [3]: model, optimizer, eng_vocab, chi_vocab, epoch, loss = load_model('best_translator.pth', device)
<ipython-input-1-83e2f4c6229d>:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(filename, map_location=device)


In [6]: translate_sentence(model, "Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berli
   ...: n Wall.", eng_vocab, chi_vocab, device)
Out[6]: '当然，在的的的的的的的的的的的。'

In [14]: translate_sentence(model, "You actually have to implement the solution", eng_vocab, chi_vocab, device)
Out[14]: '我们的的的的。'

(inline-completion-model) ➜  poc git:(main) ✗ du -sh  best_translator.pth
212M	best_translator.pth

